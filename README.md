# Databricks ETL Pipeline

Pipeline ETL bÃ¡sico para procesar datos de ventas usando PySpark y Delta Lake en Databricks, con deployment automatizado mediante Databricks Asset Bundles.

## ğŸ“Š DescripciÃ³n del Proyecto

Este proyecto implementa un pipeline de datos end-to-end que:
- **Ingesta** datos de ventas desde una tabla Delta
- **Transforma** los datos aplicando lÃ³gica de negocio
- **Carga** resultados en una tabla Delta optimizada

### Transformaciones aplicadas
- âœ… CÃ¡lculo de monto total por orden (`quantity Ã— price`)
- âœ… ExtracciÃ³n de componentes temporales (aÃ±o, mes, dÃ­a de la semana)
- âœ… ClasificaciÃ³n de Ã³rdenes por volumen (Small/Medium/Large)
- âœ… CategorizaciÃ³n por precio (Budget/Standard/Premium)
- âœ… Registro de timestamp de procesamiento

### MÃ©tricas generadas
- ğŸ“ˆ Ventas totales por paÃ­s
- ğŸ“¦ Ventas por categorÃ­a de producto
- ğŸ’° EstadÃ­sticas de montos (min, max, avg, total)

## ğŸ“ Estructura del Proyecto
```
databricks-etl-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Datos fuente (ventas_raw.csv)
â”‚   â””â”€â”€ processed/        # Datos transformados (local)
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ notebooks/
â”‚       â””â”€â”€ etl_pipeline.ipynb  # Notebook principal del pipeline
â”œâ”€â”€ src/                  # CÃ³digo reutilizable (prÃ³ximas iteraciones)
â”œâ”€â”€ tests/                # Pruebas unitarias (prÃ³ximas iteraciones)
â”œâ”€â”€ docs/                 # DocumentaciÃ³n adicional
â”œâ”€â”€ databricks.yml        # ConfiguraciÃ³n del Databricks Asset Bundle
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ requirements.txt
```

## ğŸš€ CÃ³mo Ejecutar

### Prerrequisitos
- Cuenta de Databricks (Free Tier o superior)
- Databricks CLI instalada
- VS Code con extensiÃ³n de Databricks

### Setup inicial

1. **Clonar el repositorio**:
```bash
git clone https://github.com/msosasoto/databricks-etl-pipeline.git
cd databricks-etl-pipeline
```

2. **Configurar Databricks CLI**:
```bash
databricks auth login --host <TU_WORKSPACE_URL>
```

3. **Subir datos iniciales**:
   - Cargar `data/raw/ventas_raw.csv` a Databricks como tabla Delta
   - CatÃ¡logo: `datasets_github_projects`
   - Schema: `default`
   - Tabla: `ventas_raw`

### Desarrollo y Deployment

1. **Editar el notebook localmente** en VS Code:
```bash
code resources/notebooks/etl_pipeline.ipynb
```

2. **Validar configuraciÃ³n**:
```bash
databricks bundle validate
```

3. **Deployar a Databricks**:
```bash
databricks bundle deploy
```

4. **Ejecutar el Job** desde Databricks Web:
   - Ve a **Workflows** â†’ `[dev] ETL Pipeline`
   - Clic en **Run Now**

### Tablas generadas
- ğŸ“¥ **Entrada**: `datasets_github_projects.default.ventas_raw`
- ğŸ“¤ **Salida**: `datasets_github_projects.default.ventas_transformed`

## ğŸ› ï¸ Stack TecnolÃ³gico

| Herramienta | VersiÃ³n | PropÃ³sito |
|------------|---------|-----------|
| **Databricks** | Free Tier | Plataforma de procesamiento |
| **PySpark** | 3.5+ | Motor de transformaciones |
| **Delta Lake** | 3.0+ | Almacenamiento transaccional |
| **Python** | 3.10+ | Lenguaje base |
| **Databricks CLI** | 0.275+ | Deployment automatizado |
| **Databricks Asset Bundles** | - | Infraestructura como cÃ³digo |

## ğŸ“Š Resultados

### Dataset Original
- 10 registros de ventas
- 8 columnas
- Rango: 2024-01-15 a 2024-01-19

### Dataset Transformado
- 10 registros enriquecidos
- 15 columnas (7 nuevas columnas calculadas)
- Formato: Delta Lake con versionamiento

### MÃ©tricas de Negocio
- **Revenue total**: $3,654.88
- **Ticket promedio**: $365.49
- **CategorÃ­a top**: Electronics (60% de Ã³rdenes)

## ğŸ”„ Flujo de Desarrollo
```
Editar en VS Code â†’ databricks bundle deploy â†’ Ejecutar en Databricks
```

Los cambios en `resources/notebooks/etl_pipeline.ipynb` se sincronizan automÃ¡ticamente con el workspace de Databricks mediante el Bundle.

## ğŸ¯ PrÃ³ximos Pasos

- [ ] Implementar pruebas unitarias con pytest
- [ ] Agregar validaciÃ³n de calidad de datos
- [ ] Crear pipeline orquestado con Databricks Workflows
- [ ] Implementar CI/CD con GitHub Actions
- [ ] AÃ±adir logging estructurado

## ğŸ‘¤ Autor

**Mariano Sosa**  
Ingeniero de Datos  
Stack: Python | PySpark | Azure | Databricks | Docker

---

ğŸ“ *Este es el Proyecto #1 de una serie de proyectos incrementales en ingenierÃ­a de datos*